{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mahla Entezari 401222017\n",
    "# Assignment 2\n",
    "# Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)Write the exact formulation of Bias-Variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with high variance pays a lot of attention to training data and doesn't generalize on the data which it hasn't seen before.  (Overfitting)\\\n",
    "It performs very well on training data and high error on test data.\\\n",
    "##### Variance[f(x)]=E[x^2]-(E[x]^2) \n",
    "\n",
    "Bias is differnce between the average prediction of our model and the correct value.\\\n",
    "Model with high bias pays very little attention to training data and oversimplifies the model and have high error on training and test data.  (Underfitting)\\\n",
    "##### Bias[f'(x)]=E[f'(x)-f(x)] \n",
    "\n",
    "#### Err(x)=Bias^2 + Variance + Irreducable Error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Under what conditions might logistic regression outperform linear discriminant analysis, and vice versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a classification algorithm traditionally limited to only two-class classification problems.\\\n",
    "If we have more than two classes then Linear Discriminant Analysis is the preferred linear classification technique.\n",
    "\n",
    "Limitations of Logistic regression:\\\n",
    "Logistic regression is intended for two-class or binary classification problems.\\\n",
    "Logistic regression can become unstable when the classes are well separated or When there are few examples from which to estimate the parameters.\n",
    "\n",
    "When the number of data is small and the data distribution in each class is almost normal, the linear discriminant analysis method is better.\\\n",
    "Linear discriminant analysis method is more famous for multi-class problems.\n",
    "\n",
    "When the data distribution is not normal, or the number of data is large, or when we have only two classes, Logistic regression is better tha LDA.\\\n",
    "LDA is sensitive to outliers because it involves estimating means and covariance matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Discuss the impact of the curse of dimensionality on the K-Nearest Neighbor algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any machine learning algorithms which are based on the distance measure including KNN tend to fail when the number of dimensions in the data is very high.\\\n",
    "Thus, dimensionality can be considered as a curse in such algorithms.\\\n",
    "In high-dimensional spaces, the notion of proximity or similarity becomes less meaningful.\\\n",
    "This can negatively impact the performance of KNN, as the algorithm relies on the assumption that similar instances are close to each other.\\\n",
    "In large datasets with high-dimensional feature spaces, the accuracy and effectiveness of KNN may decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Explain the concept of nested cross-validation and its benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross-validation means that we have two for loops instead of one.\\\n",
    "An outer loop for choosing test set from train set (assessing the quality of the model) , and inner loop is for choosing validation set from remained trainig set (model/parameter selection). Those loops are independent.\n",
    "\n",
    "##### Benefits :\n",
    "Nested cross-validation provides an unbiased estimate of the model performance, as the hyperparameter tuning is done independently in each iteration of the outer loop.\\\n",
    " It helps in selecting the hyperparameters that generalize well across different subsets of the data, rather than fitting to a specific train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) If we use a combination of the penalty term in Lasso and Ridge regression, what will happen in classification. Describe that.\n",
    "### Also compare norm-one and norm-two and write about the intuition. If we replace Norm-p or norm-infinity instead of norm-1 or norm-2, what is the result?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Lasso and Ridge penalties in classification through Elastic Net finds a middle ground between picking important features and handling data that's closely related. Lasso focuses on only a few important features, while Ridge keeps all features but makes them smaller.\\\n",
    "Changing norm-one or norm-two to other norms like norm-p or norm-infinity gives different ways to adjust the model, depending on what you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) How can the bootstrap method be used to estimate the standard error of a sample mean? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrap method for estimating the standard error of a sample mean involves repeatedly sampling from the original data with replacement, calculating the mean for each bootstrap sample, and then estimating the standard deviation of these bootstrap sample means. This empirical distribution of sample means provides an estimate of the variability in the original sample mean. The standard error is then calculated as the standard deviation of the bootstrap sample means. By resampling from the original data, the bootstrap method captures the uncertainty in the sample mean due to variability in the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g) Discuss how cross-validation can be used to assess the bias-variance trade-off of a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation evaluates a model's performance by partitioning data, revealing if it's underfitting (high bias) or overfitting (high variance). By analyzing metrics across folds, it helps in finding the right model complexity that balances bias and variance, ensuring better generalization to new data. This iterative process aids in selecting models with optimal performance, essential for effective machine learning model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
